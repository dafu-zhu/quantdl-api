{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuantDL Tutorial\n",
    "\n",
    "A comprehensive guide to using QuantDL for alpha research.\n",
    "\n",
    "**Contents:**\n",
    "1. Setup & Configuration\n",
    "2. Security Resolution\n",
    "3. Daily Price Data\n",
    "4. Fundamentals & Metrics\n",
    "5. Time-Series Operators\n",
    "6. Cross-Sectional Operators\n",
    "7. Alpha Factor Example\n",
    "8. Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Import quantdl\n",
    "from quantdl import QuantDLClient, SecurityInfo\n",
    "from quantdl.operators import (\n",
    "    # Time-series\n",
    "    ts_mean, ts_sum, ts_std, ts_min, ts_max, ts_delta, ts_delay,\n",
    "    # Cross-sectional\n",
    "    rank, zscore, demean, scale\n",
    ")\n",
    "import polars as pl\n",
    "from datetime import date\n",
    "\n",
    "# Initialize client\n",
    "client = QuantDLClient()\n",
    "print(f\"QuantDL version: {__import__('quantdl').__version__}\")\n",
    "print(\"Client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal: Client Architecture\n",
    "\n",
    "```\n",
    "QuantDLClient\n",
    "     |\n",
    "     +---> S3StorageBackend ---> Polars scan_parquet() ---> S3 bucket\n",
    "     |          |\n",
    "     |          +---> storage_options (AWS credentials)\n",
    "     |\n",
    "     +---> DiskCache ---> ~/.quantdl/cache/\n",
    "     |          |\n",
    "     |          +---> metadata.json (LRU tracking)\n",
    "     |          +---> data/*.parquet (cached files)\n",
    "     |\n",
    "     +---> SecurityMaster ---> PIT symbol resolution\n",
    "```\n",
    "\n",
    "**Code references:**\n",
    "- `client.py:37-75` - Client initialization\n",
    "- `storage/s3.py:20-52` - S3 backend with Polars native integration\n",
    "- `storage/cache.py:28-57` - Cache initialization with LRU/TTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Security Resolution\n",
    "\n",
    "Resolve symbols, CIKs, or security IDs to `SecurityInfo` with point-in-time accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve single symbol\n",
    "info = client.resolve(\"AAPL\")\n",
    "if info:\n",
    "    print(f\"Symbol: {info.symbol}\")\n",
    "    print(f\"Security ID: {info.security_id}\")\n",
    "    print(f\"Company: {info.company}\")\n",
    "    print(f\"CIK: {info.cik}\")\n",
    "    print(f\"CUSIP: {info.cusip}\")\n",
    "    print(f\"PERMNO: {info.permno}\")\n",
    "else:\n",
    "    print(\"AAPL not found in security master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve by CIK (requires v0.1.1+ for int64 CIK columns)\n",
    "try:\n",
    "    by_cik = client.resolve(\"0000320193\")  # Apple's CIK\n",
    "    print(f\"Resolved by CIK: {by_cik.symbol if by_cik else 'Not found'}\")\n",
    "except Exception as e:\n",
    "    print(f\"CIK resolution error (update to v0.1.1+): {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by partial name\n",
    "try:\n",
    "    results = client.security_master.search(\"Apple\", limit=5)\n",
    "    print(f\"Found {len(results)} results:\")\n",
    "    for r in results:\n",
    "        print(f\"  {r.symbol}: {r.company}\")\n",
    "except Exception as e:\n",
    "    print(f\"Search error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal: Point-in-Time Resolution\n",
    "\n",
    "```\n",
    "resolve(identifier, as_of)\n",
    "     |\n",
    "     +---> Load security_master.parquet (cached)\n",
    "     |\n",
    "     +---> Filter: start_date <= as_of AND (end_date IS NULL OR end_date >= as_of)\n",
    "     |\n",
    "     +---> Match identifier against: symbol, security_id, cik, cusip, permno\n",
    "     |          (v0.1.1+: cast to string for comparison)\n",
    "     |\n",
    "     +---> Return SecurityInfo or None\n",
    "```\n",
    "\n",
    "**Code references:**\n",
    "- `data/security_master.py:60-101` - resolve() with PIT filtering\n",
    "- `data/security_master.py:87-88` - Cast to Utf8 for type-safe comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Daily Price Data\n",
    "\n",
    "Fetch OHLCV data as wide tables (dates as rows, symbols as columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to fetch daily data from S3\n",
    "try:\n",
    "    prices = client.daily(\n",
    "        [\"AAPL\", \"MSFT\", \"GOOGL\"],\n",
    "        field=\"close\",\n",
    "        start=\"2024-01-01\",\n",
    "        end=\"2024-03-31\"\n",
    "    )\n",
    "    print(f\"Fetched from S3: {prices.shape}\")\n",
    "    print(prices.head())\n",
    "    USE_MOCK_DATA = False\n",
    "except Exception as e:\n",
    "    print(f\"S3 data not available: {type(e).__name__}\")\n",
    "    print(\"Using mock data for operator examples...\")\n",
    "    USE_MOCK_DATA = True\n",
    "    \n",
    "    # Create mock price data\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    dates = pl.date_range(date(2024, 1, 1), date(2024, 3, 31), eager=True)\n",
    "    n = len(dates)\n",
    "    prices = pl.DataFrame({\n",
    "        \"timestamp\": dates,\n",
    "        \"AAPL\": [185.0 + sum(random.gauss(0.5, 2) for _ in range(i)) for i in range(n)],\n",
    "        \"MSFT\": [375.0 + sum(random.gauss(0.3, 1.5) for _ in range(i)) for i in range(n)],\n",
    "        \"GOOGL\": [140.0 + sum(random.gauss(0.2, 1.8) for _ in range(i)) for i in range(n)],\n",
    "    })\n",
    "    print(f\"Mock data shape: {prices.shape}\")\n",
    "    print(prices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal: Daily Data Fetching\n",
    "\n",
    "```\n",
    "daily(symbols, field, start, end)\n",
    "     |\n",
    "     +---> Resolve symbols to security_ids via SecurityMaster\n",
    "     |\n",
    "     +---> Async parallel fetch (ThreadPoolExecutor, max_concurrency=10)\n",
    "     |          |\n",
    "     |          +---> For each security_id:\n",
    "     |                   +---> Check cache\n",
    "     |                   +---> If miss: S3 read data/raw/ticks/daily/{security_id}/history.parquet\n",
    "     |                   +---> Filter by date range\n",
    "     |\n",
    "     +---> Build wide table:\n",
    "     |          +---> Tag each df with symbol\n",
    "     |          +---> Concat all: (timestamp, symbol, value)\n",
    "     |          +---> Pivot: rows=timestamp, cols=symbol\n",
    "     |\n",
    "     +---> Sort by timestamp, return\n",
    "```\n",
    "\n",
    "**Code references:**\n",
    "- `client.py:167-235` - daily() method\n",
    "- `client.py:139-165` - Async parallel fetch\n",
    "- `client.py:107-137` - Single security fetch with caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Fundamentals & Metrics\n",
    "\n",
    "Fetch SEC filing fundamentals and derived metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamentals (requires data in S3)\n",
    "try:\n",
    "    revenue = client.fundamentals([\"AAPL\"], concept=\"Revenue\", start=\"2023-01-01\", end=\"2024-06-30\")\n",
    "    print(\"Revenue data:\")\n",
    "    print(revenue)\n",
    "except Exception as e:\n",
    "    print(f\"Fundamentals not available: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics (requires data in S3)\n",
    "try:\n",
    "    pe = client.metrics([\"AAPL\"], metric=\"pe_ratio\", start=\"2023-01-01\", end=\"2024-06-30\")\n",
    "    print(\"PE Ratio data:\")\n",
    "    print(pe)\n",
    "except Exception as e:\n",
    "    print(f\"Metrics not available: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal: Fundamentals/Metrics Fetching\n",
    "\n",
    "```\n",
    "fundamentals(symbols, concept, start, end)\n",
    "     |\n",
    "     +---> Resolve symbols to SecurityInfo (need CIK)\n",
    "     |\n",
    "     +---> Async parallel fetch by CIK:\n",
    "     |          +---> S3 read data/raw/fundamental/{cik}/fundamental.parquet\n",
    "     |\n",
    "     +---> Filter by concept, pivot to wide table\n",
    "\n",
    "metrics(symbols, metric, start, end)\n",
    "     |\n",
    "     +---> Same flow, but reads from:\n",
    "              data/derived/features/fundamental/{cik}/metrics.parquet\n",
    "```\n",
    "\n",
    "**Code references:**\n",
    "- `client.py:292-348` - fundamentals()\n",
    "- `client.py:404-458` - metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Time-Series Operators\n",
    "\n",
    "Column-wise operations applied over time (down each column).\n",
    "\n",
    "These operators work on wide DataFrames where:\n",
    "- First column = timestamp\n",
    "- Remaining columns = symbol values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Working with prices data: {prices.shape}\")\n",
    "print(prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_mean: Rolling mean (moving average)\n",
    "ma_20 = ts_mean(prices, 20)\n",
    "print(\"20-day moving average:\")\n",
    "print(ma_20.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_std: Rolling standard deviation (volatility)\n",
    "volatility = ts_std(prices, 20)\n",
    "print(\"20-day rolling std (volatility):\")\n",
    "print(volatility.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_delta: Difference (daily change)\n",
    "daily_change = ts_delta(prices, 1)\n",
    "print(\"1-day price change:\")\n",
    "print(daily_change.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_delay: Lag values (shift backward)\n",
    "lagged = ts_delay(prices, 5)\n",
    "print(\"Prices lagged by 5 days:\")\n",
    "print(lagged.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_min, ts_max: Rolling min/max\n",
    "rolling_high = ts_max(prices, 20)\n",
    "rolling_low = ts_min(prices, 20)\n",
    "print(\"20-day rolling high:\")\n",
    "print(rolling_high.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal: Time-Series Operators\n",
    "\n",
    "```\n",
    "ts_mean(df, window)\n",
    "     |\n",
    "     +---> Identify: date_col = columns[0], value_cols = columns[1:]\n",
    "     |\n",
    "     +---> For each value column:\n",
    "     |          +---> Apply pl.col(c).rolling_mean(window)\n",
    "     |\n",
    "     +---> Return df with same structure\n",
    "```\n",
    "\n",
    "| Operator | Polars Function |\n",
    "|----------|----------------|\n",
    "| `ts_mean` | `rolling_mean` |\n",
    "| `ts_sum` | `rolling_sum` |\n",
    "| `ts_std` | `rolling_std` |\n",
    "| `ts_min` | `rolling_min` |\n",
    "| `ts_max` | `rolling_max` |\n",
    "| `ts_delta` | `diff` |\n",
    "| `ts_delay` | `shift` |\n",
    "\n",
    "**Code references:**\n",
    "- `operators/timeseries.py:10-85` - All 7 TS operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Cross-Sectional Operators\n",
    "\n",
    "Row-wise operations applied across symbols (across each row at each date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank: Cross-sectional rank (1 to N)\n",
    "ranked = rank(prices)\n",
    "print(\"Cross-sectional rank (each row: ranks 1,2,3 -> sum=6):\")\n",
    "print(ranked.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zscore: Standardize across symbols (mean=0, std=1 per row)\n",
    "standardized = zscore(prices)\n",
    "print(\"Cross-sectional z-score:\")\n",
    "print(standardized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demean: Subtract row mean\n",
    "demeaned = demean(prices)\n",
    "print(\"Demeaned (each row sums to ~0):\")\n",
    "print(demeaned.head())\n",
    "\n",
    "# Verify: sum across columns should be ~0\n",
    "row_sums = demeaned.select(pl.sum_horizontal(pl.exclude(\"timestamp\"))).to_series()\n",
    "print(f\"\\nRow sums (should be ~0): {row_sums.head().to_list()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale: Scale so |sum| = target (for dollar-neutral portfolios)\n",
    "weights = scale(demeaned, target=1.0)\n",
    "print(\"Scaled weights (|sum| = 1):\")\n",
    "print(weights.head())\n",
    "\n",
    "# Verify: sum of absolute values should be ~1\n",
    "abs_sums = weights.select(pl.sum_horizontal(*[pl.col(c).abs() for c in weights.columns[1:]])).to_series()\n",
    "print(f\"\\nAbs sums (should be ~1): {abs_sums.head().to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal: Cross-Sectional Operators\n",
    "\n",
    "**rank():** Uses unpivot/pivot pattern\n",
    "```\n",
    "rank(df)\n",
    "     |\n",
    "     +---> Unpivot to long: (timestamp, symbol, value)\n",
    "     |\n",
    "     +---> Rank within each timestamp: pl.col(\"value\").rank().over(\"timestamp\")\n",
    "     |\n",
    "     +---> Pivot back to wide format\n",
    "```\n",
    "\n",
    "**zscore(), demean(), scale():** Use horizontal operations\n",
    "```\n",
    "zscore(df)\n",
    "     |\n",
    "     +---> row_mean = pl.mean_horizontal(*value_cols)\n",
    "     |\n",
    "     +---> row_std = pl.concat_list(...).list.eval(pl.element().std())\n",
    "     |\n",
    "     +---> Transform: (value - mean) / std for each column\n",
    "```\n",
    "\n",
    "**Code references:**\n",
    "- `operators/crosssectional.py:16-50` - rank()\n",
    "- `operators/crosssectional.py:53-76` - zscore()\n",
    "- `operators/crosssectional.py:79-97` - demean()\n",
    "- `operators/crosssectional.py:100-121` - scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Alpha Factor Example\n",
    "\n",
    "Build a simple **20-day momentum** factor and create **dollar-neutral portfolio weights**.\n",
    "\n",
    "### Strategy Concept\n",
    "Stocks that went up recently tend to continue going up (momentum effect).\n",
    "\n",
    "### Pipeline\n",
    "1. Calculate 20-day returns (momentum)\n",
    "2. Rank stocks by momentum cross-sectionally\n",
    "3. Convert ranks to z-scores (center around 0)\n",
    "4. Scale to dollar-neutral weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expand to more symbols for better demonstration\nif USE_MOCK_DATA:\n    import random\n    random.seed(42)\n    symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"JPM\", \"V\", \"JNJ\"]\n    dates = pl.date_range(date(2024, 1, 1), date(2024, 6, 30), eager=True)\n    n = len(dates)\n    \n    data = {\"timestamp\": dates}\n    base_prices = {\"AAPL\": 185.0, \"MSFT\": 375.0, \"GOOGL\": 140.0, \"AMZN\": 150.0, \"NVDA\": 500.0,\n                   \"META\": 350.0, \"TSLA\": 250.0, \"JPM\": 170.0, \"V\": 275.0, \"JNJ\": 160.0}\n    for sym in symbols:\n        drift = random.uniform(-0.1, 0.5)  # Random drift\n        vol = random.uniform(1.0, 3.0)     # Random volatility\n        data[sym] = [base_prices[sym] + sum(random.gauss(drift, vol) for _ in range(i)) for i in range(n)]\n    \n    prices = pl.DataFrame(data)\n\nprint(f\"Price data: {prices.shape}\")\nprint(f\"Symbols: {prices.columns[1:]}\")\nprint(prices.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate 20-day momentum (return)\n",
    "# Momentum = (price_today - price_20d_ago) / price_20d_ago\n",
    "\n",
    "price_20d_ago = ts_delay(prices, 20)\n",
    "value_cols = prices.columns[1:]\n",
    "\n",
    "# Calculate returns\n",
    "momentum_data = {\"timestamp\": prices[\"timestamp\"]}\n",
    "for col in value_cols:\n",
    "    momentum_data[col] = (prices[col] - price_20d_ago[col]) / price_20d_ago[col]\n",
    "momentum = pl.DataFrame(momentum_data)\n",
    "\n",
    "print(\"20-day momentum (returns):\")\n",
    "print(momentum.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Rank momentum cross-sectionally\n",
    "# Higher rank = higher momentum = we want to be long\n",
    "\n",
    "momentum_rank = rank(momentum)\n",
    "print(f\"Momentum ranks (1-{len(value_cols)}):\")\n",
    "print(momentum_rank.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert ranks to z-scores\n",
    "# This centers around 0 (some positive, some negative)\n",
    "\n",
    "alpha = zscore(momentum_rank)\n",
    "print(\"Alpha (z-scored ranks):\")\n",
    "print(alpha.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Scale to dollar-neutral weights\n",
    "# |sum of weights| = 1\n",
    "\n",
    "weights = scale(alpha, target=1.0)\n",
    "print(\"Portfolio weights (positive=long, negative=short):\")\n",
    "print(weights.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dollar-neutrality\n",
    "last_row = weights.tail(1)\n",
    "weight_values = [last_row[col][0] for col in weights.columns[1:]]\n",
    "\n",
    "print(f\"Sum of weights: {sum(weight_values):.6f} (should be ~0)\")\n",
    "print(f\"Sum of |weights|: {sum(abs(w) for w in weight_values):.6f} (should be ~1)\")\n",
    "print(f\"\\nLong positions: {sum(1 for w in weight_values if w > 0)}\")\n",
    "print(f\"Short positions: {sum(1 for w in weight_values if w < 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Pipeline Summary\n",
    "\n",
    "```\n",
    "prices\n",
    "   |\n",
    "   +---> ts_delay(20) ---> price_20d_ago\n",
    "   |                            |\n",
    "   +------- (prices - price_20d_ago) / price_20d_ago -----> momentum\n",
    "                                                                |\n",
    "                                                           rank()\n",
    "                                                                |\n",
    "                                                           zscore()\n",
    "                                                                |\n",
    "                                                           scale(target=1)\n",
    "                                                                |\n",
    "                                                           weights\n",
    "                                                    (dollar-neutral portfolio)\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **Positive weight** = Long position (bet stock goes up)\n",
    "- **Negative weight** = Short position (bet stock goes down)\n",
    "- **Dollar-neutral** = Equal $ long and short (market-neutral, profit from relative performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Caching\n",
    "\n",
    "Monitor and manage the local disk cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cache statistics\n",
    "stats = client.cache_stats()\n",
    "print(f\"Cached entries: {stats['entries']}\")\n",
    "print(f\"Cache size: {stats['total_size_bytes'] / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Max size: {stats['max_size_bytes'] / 1024 / 1024 / 1024:.1f} GB\")\n",
    "print(f\"TTL: {stats['ttl_seconds'] / 3600:.1f} hours\")\n",
    "print(f\"Location: {stats['cache_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache if needed (uncomment to run)\n",
    "# client.clear_cache()\n",
    "# print(\"Cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal: Cache Architecture\n",
    "\n",
    "```\n",
    "~/.quantdl/cache/\n",
    "     |\n",
    "     +---> metadata.json\n",
    "     |          {\n",
    "     |            \"s3_path\": {\n",
    "     |              \"local_path\": \"data/abc123.parquet\",\n",
    "     |              \"size_bytes\": 1024000,\n",
    "     |              \"fetched_at\": 1704067200.0,\n",
    "     |              \"last_accessed\": 1704153600.0\n",
    "     |            }\n",
    "     |          }\n",
    "     |\n",
    "     +---> data/\n",
    "              +---> {hash}.parquet\n",
    "              ...\n",
    "```\n",
    "\n",
    "**Eviction policies:**\n",
    "- **TTL (Time-to-Live):** Entries older than 24h evicted on next access\n",
    "- **LRU (Least Recently Used):** When cache exceeds 10GB, oldest-accessed removed first\n",
    "\n",
    "**Code references:**\n",
    "- `storage/cache.py:97-121` - LRU and TTL eviction\n",
    "- `storage/cache.py:123-169` - get() with cache hit/miss\n",
    "- `storage/cache.py:171-200` - put() with atomic writes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Data APIs\n",
    "\n",
    "| API | Description | Returns |\n",
    "|-----|-------------|--------|\n",
    "| `client.resolve(id)` | Symbol/CIK/CUSIP lookup | `SecurityInfo` |\n",
    "| `client.daily(symbols, field)` | OHLCV prices | Wide DataFrame |\n",
    "| `client.fundamentals(symbols, concept)` | SEC fundamentals | Wide DataFrame |\n",
    "| `client.metrics(symbols, metric)` | Derived metrics | Wide DataFrame |\n",
    "\n",
    "### Operators\n",
    "\n",
    "| Operator | Type | Description |\n",
    "|----------|------|-------------|\n",
    "| `ts_mean(df, n)` | Time-series | Rolling mean |\n",
    "| `ts_std(df, n)` | Time-series | Rolling std |\n",
    "| `ts_delta(df, n)` | Time-series | Difference |\n",
    "| `ts_delay(df, n)` | Time-series | Lag |\n",
    "| `rank(df)` | Cross-sectional | Rank 1 to N |\n",
    "| `zscore(df)` | Cross-sectional | Standardize |\n",
    "| `demean(df)` | Cross-sectional | Center around 0 |\n",
    "| `scale(df, target)` | Cross-sectional | Normalize weights |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "client.close()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}